{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE STUDY : LOAN PREDICTION ANALYTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Information\n",
    "\n",
    "   \n",
    "Variable | Description\n",
    "----------|--------------\n",
    "Loan_ID | Unique Loan ID\n",
    "Gender | Male/ Female\n",
    "Married | Applicant married (Y/N)\n",
    "Dependents | Number of dependents\n",
    "Education | Applicant Education (Graduate/ Under Graduate)\n",
    "Self_Employed | Self employed (Y/N)\n",
    "ApplicantIncome | Applicant income\n",
    "CoapplicantIncome | Coapplicant income\n",
    "LoanAmount | Loan amount in thousands\n",
    "Loan_Amount_Term | Term of loan in months\n",
    "Credit_History | credit history meets guidelines\n",
    "Property_Area | Urban/ Semi Urban/ Rural\n",
    "Loan_Status | Loan approved (Y/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules Load Essential Python Libraries\n",
    "\n",
    "# Data Engineering Part \n",
    "\n",
    "Important Libaries for Data Engineering Tasks\n",
    "\n",
    "1- Numpy ( Numerical Python ) 2- Pandas 3- Matplotlib 4- Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NUMPY:\n",
    "NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays https://numpy.org/\n",
    "\n",
    "# What is Pandas:\n",
    "pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.\n",
    "\n",
    "https://pandas.pydata.org/docs/index.html\n",
    "\n",
    "https://www.w3schools.com/python/pandas/pandas_dataframes.asp\n",
    "\n",
    "https://www.tutorialspoint.com/python_pandas/python_pandas_dataframe.htm\n",
    "\n",
    "# What is Matplotlib:\n",
    "\n",
    "Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK. https://matplotlib.org/\n",
    "\n",
    "# What is Seaborn:\n",
    "Seaborn is a library for making statistical graphics in Python. It builds on top of matplotlib and integrates closely with pandas data structures. Seaborn helps you explore and understand your data. https://seaborn.pydata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://research.google.com/colaboratory/\n",
    "#from google.colab import drive  \n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Create Dataframe through Pandas\n",
    "DataFrame. DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object.\n",
    "\n",
    "https://www.w3schools.com/python/pandas/pandas_dataframes.asp\n",
    "\n",
    "load comma separeted variable (CSV ) data with pd.read_csv method\n",
    "\n",
    "https://sites.google.com/a/umt.edu.pk/datascience/data-engineering-pundas\n",
    "\n",
    "In above link you will find this code file regarding various examples to crearte data frame ...> downlaod this file ( 02- PANDAS DATAFRAMES.IPYNB )\n",
    "\n",
    "# Youtube Videos \n",
    "Tutorial 5- Pandas, Data Frame and Data Series Part-1\n",
    "\n",
    "https://www.youtube.com/watch?v=QUClKFFn1Vk&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=9\n",
    "\n",
    "Tutorial 6- Pandas,Reading CSV files With Various Parameters-\n",
    "\n",
    "https://www.youtube.com/watch?v=tW1BWtQRZ2M&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  \n",
       "3             1.0         Urban           Y  \n",
       "4             1.0         Urban           Y  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Loan Prediction Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Categorical Columns:\n",
    "\n",
    "Gender (Male/Female),\n",
    "Married (Yes/No),\n",
    "Number of dependents (Possible values:0,1,2,3+),\n",
    "Education (Graduate / Not Graduate),\n",
    "Self-Employed (No/Yes),\n",
    "credit history(Yes/No),\n",
    "Property Area (Rural/Semi-Urban/Urban) and\n",
    "Loan Status (Y/N)(i. e. Target variable)\n",
    "\n",
    "#Numerical Columns:\n",
    "\n",
    "Loan ID,\n",
    "Applicant Income,\n",
    "Co-applicant Income,\n",
    "Loan Amount, and\n",
    "Loan amount term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "- Need to get a better understanding of the given data.\n",
    "- Questions like:\n",
    "- How much data do we have?\n",
    "- Are there are any missing values?\n",
    "- What is the data type of each column?\n",
    "- What is the distribution of data in each column?\n",
    "- Do we see any outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary \n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows: \", df.shape[0])\n",
    "counts = df.describe().iloc[0]\n",
    "display(pd.DataFrame(counts.tolist(), columns=[\"Count of values\"], index=counts.index.values).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Wrangling \n",
    "\n",
    "# Handling Missing Values & Outlier Handling\n",
    "\n",
    "In order to check missing values in Pandas DataFrame, we use a function isnull() and notnull() . Both function help in checking whether a value is NaN or not. These function can also be used in Pandas Series in order to find null values in a series\n",
    "\n",
    "\n",
    "# Useful blogs \n",
    "\n",
    "https://towardsdatascience.com/8-methods-for-handling-missing-values-with-python-pandas-842544cdf891\n",
    "\n",
    "https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/\n",
    "\n",
    "https://www.tutorialspoint.com/python_pandas/python_pandas_missing_data.htm\n",
    "\n",
    "https://sites.google.com/a/umt.edu.pk/datascience/data-engineering-pundas \n",
    "\n",
    "Download this file for practice \n",
    "\n",
    "# Youtube Video \n",
    "\n",
    "https://www.youtube.com/watch?v=uDr67HBIPz8\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=fCMrO_VzeL8\n",
    "\n",
    "https://www.youtube.com/watch?v=EaGbS7eWSs0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap to check missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing values for numerical terms - mean\n",
    "df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].mean())\n",
    "df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mean())\n",
    "df['Credit_History'] = df['Credit_History'].fillna(df['Credit_History'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the missing values for categorical terms - mode\n",
    "df['Gender'] = df[\"Gender\"].fillna(df['Gender'].mode()[0])\n",
    "df['Married'] = df[\"Married\"].fillna(df['Married'].mode()[0])\n",
    "df['Dependents'] = df[\"Dependents\"].fillna(df['Dependents'].mode()[0])\n",
    "df['Self_Employed'] = df[\"Self_Employed\"].fillna(df['Self_Employed'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Pandas Groupby\n",
    "\n",
    "Pandas groupby is used for grouping the data according to the categories and apply a function to the categories.\n",
    "\n",
    "It also helps to aggregate data efficiently.\n",
    "\n",
    "Pandas dataframe. groupby() function is used to split the data into groups based on some criteria.\n",
    "\n",
    "pandas objects can be split on any of their axes.\n",
    "\n",
    "What does a groupby do?\n",
    "Group by is one of the most frequently used SQL clauses. It allows you to collapse a field into its distinct values.\n",
    "\n",
    "# Useful links\n",
    "\n",
    "https://www.tutorialspoint.com/python_pandas/python_pandas_groupby.htm\n",
    "\n",
    "https://sites.google.com/a/umt.edu.pk/datascience/data-engineering-pundas --- >\n",
    "\n",
    "download file 04- PANDAS GROUPBY.IPYNB file\n",
    "\n",
    "PANDASDS.IPYNB file download with Data.rar folder as well\n",
    "\n",
    "# Youtube Video\n",
    "\n",
    "Python Pandas Tutorial 7. Group By (Split Apply Combine)\n",
    "\n",
    "https://www.youtube.com/watch?v=Wb2Tp35dZ-I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by command to find mean of multiple colums \n",
    "df_Loan_Loan_Amount_Term =df.groupby([\"LoanAmount\",\"ApplicantIncome\"],as_index=False).Loan_Amount_Term.mean()\n",
    "\n",
    "print(df_Loan_Loan_Amount_Term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization \n",
    "Data visualization is defined as a graphical representation that contains the information and the data. By using visual elements like charts, graphs, and maps, data visualization techniques provide an accessible way to see and understand trends, outliers, and patterns in data\n",
    "\n",
    "# UseFull links of Matplotlib to plot through rcParams\n",
    "https://matplotlib.org/3.5.0/tutorials/introductory/customizing.html\n",
    "\n",
    "https://www.programcreek.com/python/example/102312/matplotlib.pyplot.rcParams\n",
    "\n",
    "https://www.tutorialexample.com/understand-matplotlib-rcparams-a-beginner-guide-matplotlib-tutorial/\n",
    "\n",
    "https://sites.google.com/a/umt.edu.pk/datascience/4-data-visualization-matplotlib-and-seaboan\n",
    "\n",
    "# Youtube Video\n",
    "\n",
    "Tutorial 8- Matplotlib (Simple Visualization Library)\n",
    "\n",
    "https://www.youtube.com/watch?v=czQO1_GEEos&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=11\n",
    "\n",
    "Tutorial 9- Seaborn Tutorial- Distplot, Joinplot, Pairplot Part 1\n",
    "\n",
    "https://www.youtube.com/watch?v=UsglokDLa2o&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Univariate Analysis:\n",
    "# categorical attributes visualization\n",
    "fig,ax = plt.subplots(2,4,figsize=(16,10))\n",
    "sns.countplot(x='Loan_Status', data = df, ax=ax[0][0])\n",
    "sns.countplot(x='Gender', data = df, ax=ax[0][1])\n",
    "sns.countplot(x='Married', data = df, ax=ax[0][2])\n",
    "sns.countplot(x='Education', data = df, ax=ax[0][3])\n",
    "sns.countplot(x='Self_Employed', data = df, ax=ax[1][0])\n",
    "sns.countplot(x='Property_Area', data = df, ax=ax[1][1])\n",
    "sns.countplot(x='Credit_History', data = df, ax=ax[1][2])\n",
    "sns.countplot(x='Dependents', data = df, ax=ax[1][3])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Univariate Analysis Observations:\n",
    "\n",
    "More Loans are approved Vs Rejected\n",
    "Count of Male applicants is more than Female\n",
    "Count of Married applicant is more than Non-married\n",
    "Count of graduate is more than non-Graduate\n",
    "Count of self-employed is less than that of Non-Self-employed\n",
    "Maximum properties are located in Semiurban areas\n",
    "Credit History is present for many applicants\n",
    "The count of applicants with several dependents=0 is maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for data imbalance\n",
    "\n",
    "The class imbalance problem typically occurs when there are many more instances of some classes than others. In such cases, standard classifiers tend to be overwhelmed by the large classes and ignore the small ones.\n",
    "\n",
    "# Useful Blobs \n",
    "https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/\n",
    "\n",
    "https://machinelearningmastery.com/what-is-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df['Loan_Status'])\n",
    "print('The percentage of Y class : %.2f' % (df['Loan_Status'].value_counts()[0] / len(df)))\n",
    "print('The percentage of N class : %.2f' % (df['Loan_Status'].value_counts()[1] / len(df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total income\n",
    "df['Total_Income'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis\n",
    "sns.boxplot(x='Loan_Status', y='Total_Income', data=df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Mean Total_Income of 0 and 1 are almost the same (o: no,1: Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Total_Income', y='Gender', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The mean value of Loan Amount applied by males (0) is slightly higher than Females(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_History = pd.crosstab(df['Credit_History'], df['Loan_Status'])\n",
    "Credit_History.div(Credit_History.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(6,4))\n",
    "plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Property_Area = pd.crosstab(df['Property_Area'], df['Loan_Status'])\n",
    "Property_Area.div(Property_Area.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(6,4))\n",
    "plt.legend(bbox_to_anchor=(1.05,1.0),loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Property_Area = pd.crosstab(df['Property_Area'], df['Loan_Status'])\n",
    "Property_Area.div(Property_Area.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(6,4))\n",
    "plt.legend(bbox_to_anchor=(1.05,1.0),loc='best')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Proportion of married applicants is higher for the approved loans.\n",
    "People with credit history as 1 are more likely to get their loan approved.\n",
    "Proportion of loans getting approved in semiurban area is higher compared to rural or urban area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability and Statistics:\n",
    "The goal of these questions is to test your ability to answer probability and stat questions with code.\n",
    "Use whatever libraries you are comfortable with.\n",
    "Code clarity and cleanliness are also highly valuable.\n",
    "\n",
    "https://sites.google.com/a/umt.edu.pk/datascience/5--statistics-and-probability\n",
    "\n",
    "https://www.kaggle.com/code/carlolepelaars/statistics-tutorial\n",
    "\n",
    "# Youtube playlist\n",
    "\n",
    "https://www.youtube.com/watch?v=dmHcFQQPGEE&list=PLVgEzPHodXi1wT9OK8B_W6Hs8Xc-gaG6N&index=1\n",
    "\n",
    "# understanding of plots Distributions plots learning of matplotlib and seaborn\n",
    "\n",
    "\n",
    "\n",
    "# Probabilty distributions from youtube\n",
    "\n",
    "Python for Data Analysis: Probability Distributions\n",
    "\n",
    "https://www.youtube.com/watch?v=uial-2girHQ\n",
    "\n",
    "Probability - Simulation to See Probability in Python\n",
    "https://www.youtube.com/watch?v=4YPt3BHuJEE\n",
    "\n",
    "3- Statistics Using Python\n",
    "\n",
    "https://www.youtube.com/watch?v=mQ-3KwrBIN0\n",
    "\n",
    "4- Tutorial 25- Probability Density function and CDF- EDA-Data Science\n",
    "\n",
    "https://www.youtube.com/watch?v=PYIjkw0HN1Q&t=2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical attributes visualization\n",
    "sns.distplot(df[\"ApplicantIncome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[\"CoapplicantIncome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[\"LoanAmount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Loan_Amount_Term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['Credit_History'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply log transformation to the attribute\n",
    "df['ApplicantIncomeLog'] = np.log(df['ApplicantIncome']+1)\n",
    "sns.distplot(df[\"ApplicantIncomeLog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CoapplicantIncomeLog'] = np.log(df['CoapplicantIncome']+1)\n",
    "sns.distplot(df[\"CoapplicantIncomeLog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['LoanAmountLog'] = np.log(df['LoanAmount']+1)\n",
    "sns.distplot(df[\"LoanAmountLog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Loan_Amount_Term_Log'] = np.log(df['Loan_Amount_Term']+1)\n",
    "sns.distplot(df[\"Loan_Amount_Term_Log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Income_Log'] = np.log(df['Total_Income']+1)\n",
    "sns.distplot(df[\"Total_Income_Log\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the probability of a customer having 1 credit History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we get the count of rows having n_kids =2, df_person_data.n_kids == 2 is the condition to filter rows\n",
    "# shape[0] return count of rows.\n",
    "a = df[df.Credit_History == 1].shape[0]\n",
    "total =df.shape[0] # getting total rows\n",
    "p_a = a/total # probabilty formula --> p(a) = event/total\n",
    "print(f\"Probability of customer having 1 Credit_History is {round(p_a,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coorelation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(corr, annot = True, cmap=\"BuPu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Feature Engineering\n",
    "- Feature space is almost unchanged¶\n",
    "- Remove irrelevant columns\n",
    "- Convert strings to boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "cols = ['ApplicantIncome', 'CoapplicantIncome', \"LoanAmount\", \"Loan_Amount_Term\", \"Total_Income\", 'Loan_ID', 'CoapplicantIncomeLog']\n",
    "df = df.drop(columns=cols, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering \n",
    "\n",
    "# Data Preprocessing with Label Encoder\n",
    "What is a label encoder? Image result for label encoder Label Encoding is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering. Let's see how to implement label encoding in Python using the scikit-learn library and also understand the challenges with label encoding.\n",
    "\n",
    "# Useful links\n",
    "https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n",
    "\n",
    "https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python/\n",
    "\n",
    "#sklearn.preprocessing.LabelEncoder\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html?highlight=labelencoder\n",
    "\n",
    "\n",
    "# Youtube video\n",
    "Label encoder example in python code\n",
    "\n",
    "https://www.youtube.com/watch?v=UtgrhBr3kTw\n",
    "\n",
    "https://www.youtube.com/watch?v=OTPz5plKb40&t=281s\n",
    "\n",
    "\n",
    "# Pre processing with Standard Scaler\n",
    "In Machine Learning, StandardScaler is used to resize the distribution of values ​​so that the mean of the observed values ​​is 0 and the standard deviation is 1.\n",
    "\n",
    "# Useful links\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standard%20scaler#sklearn.preprocessing.StandardScaler\n",
    "\n",
    "https://www.geeksforgeeks.org/standardscaler-minmaxscaler-and-robustscaler-techniques-ml/\n",
    "\n",
    "https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n",
    "\n",
    "# YOU TUBE Video\n",
    "1- Standardization Vs Normalization- Feature Scaling\n",
    "\n",
    "https://www.youtube.com/watch?v=mnKm3YP56PY&t=52s\n",
    "\n",
    "2- Using Standard Scaler to scale features\n",
    "\n",
    "https://www.youtube.com/watch?v=CYd_u_4P_lQ\n",
    "\n",
    "3- Data Preprocessing 01: StandardScaler Machine Learning\n",
    "\n",
    "https://www.youtube.com/watch?v=ZddUwo4R5ug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "cols = ['Gender',\"Married\",\"Education\",'Self_Employed',\"Property_Area\",\"Loan_Status\",\"Dependents\"]\n",
    "le = LabelEncoder()\n",
    "for col in cols:\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify input and output attributes\n",
    "X = df.drop(columns=['Loan_Status'], axis=1)\n",
    "y = df['Loan_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "# Cross Validation\n",
    "Cross-Validation is a statistical method of evaluating and comparing learning algorithms by dividing data into two segments: one used to learn or train a model and the other used to validate the model.\n",
    "\n",
    "# Useful Link\n",
    "https://www.analyticsvidhya.com/blog/2021/05/importance-of-cross-validation-are-evaluation-metrics-enough/\n",
    "\n",
    "https://towardsdatascience.com/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85\n",
    "\n",
    "#sklearn.model_selection.cross_val_score\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html?highlight=cross_val_score\n",
    "\n",
    "# You tube video\n",
    "1- Machine Learning Tutorial Python 12 - K Fold Cross Validation\n",
    "\n",
    "https://www.youtube.com/watch?v=gJo0uNL-5Qw\n",
    "\n",
    "2- What is Cross Validation and its types\n",
    "\n",
    "https://www.youtube.com/watch?v=7062skdX05Y&t=759s\n",
    "\n",
    "3- Machine Learning Fundamentals: Cross Validation\n",
    "\n",
    "https://www.youtube.com/watch?v=fSytzGwwBVw\n",
    "\n",
    "4- All Type Of Cross Validation With Python All In 1 Video\n",
    "\n",
    "https://www.youtube.com/watch?v=3fzYdnuvEfk\n",
    "\n",
    "5- Cross Validation in Scikit Learn\n",
    "\n",
    "https://www.youtube.com/watch?v=L_dQrZZjGDg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix,  roc_curve, precision_recall_curve, accuracy_score, roc_auc_score\n",
    "\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify function\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def classify(model, x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    model.fit(x_train, y_train)\n",
    "    print(\"Accuracy is\", model.score(x_test, y_test)*100)\n",
    "    # cross validation - it is used for better validation of model\n",
    "    # eg: cv-5, train-4, test-1\n",
    "    score = cross_val_score(model, x, y, cv=5)\n",
    "    print(\"Cross validation is\",np.mean(score)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "classify(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "classify(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "model = RandomForestClassifier()\n",
    "classify(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()\n",
    "classify(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify function\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def classify(modelhy, x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    modelhy.fit(x_train, y_train)\n",
    "    print(\"Accuracy is\", modelhy.score(x_test, y_test)*100)\n",
    "    # cross validation - it is used for better validation of model\n",
    "    # eg: cv-5, train-4, test-1\n",
    "    score = cross_val_score(modelhy, x, y, cv=5)\n",
    "    print(\"Cross validation is\",np.mean(score)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Random Forest\n",
    "n_estimators: The number of trees in the forest.\n",
    "\n",
    "max_depth: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples\n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
    "\n",
    "max_features: The number of features to consider when looking for the best split If int, then consider max_features features at each split. If float, then max_features is a fraction and round(max_features * n_features) features are considered at each split. If “auto”, then max_features=sqrt(n_features). If “sqrt”, then max_features=sqrt(n_features) (same as “auto”). If “log2”, then max_features=log2(n_features). If None, then max_features=n_features.\n",
    "\n",
    "max_samples: If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "\n",
    "# Choose the type of classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelhy = RandomForestClassifier(n_estimators=110, min_samples_split=15, max_depth=8, max_features=1)\n",
    "classify(modelhy, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do you evaluate your model?\n",
    "\n",
    "### - Different situations or business problems call for different metrics\n",
    "\n",
    "### - The most basic summary is given by the Confusion Matrix\n",
    "![Figure 1-1](cm.png \"Figure 1-1\")\n",
    "\n",
    "### - Type I & II Errors (False Positives / False Negatives)\n",
    "\n",
    "### - Then you can consider:\n",
    ">### - Accuracy\n",
    ">### - Precision\n",
    ">### - Recall\n",
    ">### - Lift\n",
    ">### - Support\n",
    ">### - Confidence etc.\n",
    "![Figure 1-2](metrics.png \"Figure 1-2\")\n",
    "\n",
    " ### - Many, many more metrics have been proposed:\n",
    "![Figure 1-3](many.png \"Figure 1-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelrm = RandomForestClassifier()\n",
    "modelrm.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=modelrm.predict(x_test)\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"Training Score:\\n\",modelrm.score(x_train,y_train)*100)\n",
    "print(\"Mean Squared Error:\\n\",mean_squared_error(y_test,y_pred))\n",
    "print(\"R2 score is:\\n\",r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = modelrm.predict(x_test)\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on other models ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC CURVE \n",
    "ROC curves are frequently used to show in a graphical way the connection/trade-off between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests. In addition the area under the ROC curve gives an idea about the benefit of using the test(s) in question.\n",
    "AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes.\n",
    "\n",
    "\n",
    "# How to plot ROC curve in single \n",
    "\n",
    "\n",
    "https://www.statology.org/plot-multiple-roc-curves-python/\n",
    "\n",
    "https://www.imranabdullah.com/2019-06-01/Drawing-multiple-ROC-Curves-in-a-single-plot\n",
    "\n",
    "# Youtube Video Links \n",
    "\n",
    "\n",
    "Performance Metrics(ROC,AUC Curve) For Classification Problem In Machine Learning \n",
    "https://www.youtube.com/watch?v=A_ZKMsZ3f3o\n",
    "\n",
    "How to Plot an ROC Curve in Python\n",
    "\n",
    "https://www.youtube.com/watch?v=uVJXPPrWRJ0\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=TEkvKx2tQHU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predictions of Random Forest and Logistic Regression models in the form of probability values\n",
    "y_lg_prob = model.predict_proba(x_test)[:,1]\n",
    "y_rfc_prob =  modelrm.predict_proba(x_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Logistic Regression\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,y_lg_prob)\n",
    "auc = metrics.roc_auc_score(y_test, y_lg_prob)\n",
    "\n",
    "#For Random Forest\n",
    "fpr1, tpr1, _1 = metrics.roc_curve(y_test,y_rfc_prob)\n",
    "auc1 = metrics.roc_auc_score(y_test, y_rfc_prob)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Random', alpha=.8)\n",
    "plt.plot(fpr,tpr,label=\"LR AUC = \"+str(round(auc,3)))\n",
    "plt.plot(fpr1,tpr1,label=\"RFC AUC = \"+str(round(auc1,3)))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting feature importances for Random Forest model\n",
    "(pd.Series(modelrm.feature_importances_, index=x_train.columns).plot(kind='barh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#understanding which variable(s) have the largest impact on the outcome.\n",
    "featimp = pd.Series(modelrm.feature_importances_, index=x_train.columns).sort_values(ascending=False) \n",
    "print(featimp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks \n",
    "# Contact \n",
    "\n",
    "https://www.linkedin.com/in/mazhar-javed-42587046/\n",
    "\n",
    "mazharjaved2001@yahoo.com\n",
    "\n",
    "\n",
    "\n",
    "# WhatsAPP +923334461420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
